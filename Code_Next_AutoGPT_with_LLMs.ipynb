{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deybyr647/cn-deep-learning-llms/blob/main/Code_Next_AutoGPT_with_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an AutoGPT on Colab using Python & LangChain\n",
        "\n",
        "Hey Engineers! This Colab is meant as a way to show you all just how easy it is to create your own basic application that works with Large Language Models, or rather, LLMs, as we like to call them.\n",
        "\n",
        "<br>\n",
        "\n",
        "This Colab will more or less be somehwat of a skeleton that you can go into and modify for your own use. Because we're working a lot with Python, I thought it'd be a cool idea to gear this to be like a ChatGPT that focuses on explaining Python programming concepts.\n",
        "<hr>"
      ],
      "metadata": {
        "id": "aK1p5EOn-Vw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FT2PU4XDZwzi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Things To Know\n",
        "\n",
        "Due to the cloud nature of Google Colab & how it executes code, this notebook will be broken down into various sections.\n",
        "\n",
        "<br>\n",
        "\n",
        "In each section, all the different \"pieces to the puzzle\" will be explained line by line.\n",
        "\n",
        "<br>\n",
        "\n",
        "At the end (or bottom) of the notebook, you will find one big cell that brings all the different pieces together for successful execution.\n",
        "\n",
        "<br>\n",
        "\n",
        "In breaking everything down, the goal for you as the engineer is to digest everything, piece by piece, and watch it all come together in the end. The idea will then be to have you modify or recreate this project to your liking.\n",
        "<hr>\n",
        "\n"
      ],
      "metadata": {
        "id": "2ZL38UhgjeRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Started\n",
        "\n",
        "If you haven't already, please make sure to read the above notes!\n",
        "<hr>"
      ],
      "metadata": {
        "id": "bMheYCgik9It"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Setup\n",
        "\n",
        "This section will focus on setting up your Google Colab environment to properly run your application. In other words, running the cells here is crucial.\n",
        "\n",
        "<br>\n",
        "\n",
        "The cells here will install the necessary Python dependencies for the project, along with helper dependencies that will allow you to view your app in your browser, as if you were running it locally from your device.\n",
        "\n"
      ],
      "metadata": {
        "id": "C9O8DVugc7MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary Python packages using pip\n",
        "!pip install -q langchain streamlit openai wikipedia chromadb tiktoken"
      ],
      "metadata": {
        "id": "mJmFJAlOdlXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install localtunnel to serve Streamlit App, using NPM\n",
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "E_kX2kHkd8hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package Imports\n",
        "The code below simply focuses on including the necessary packages within your Python code"
      ],
      "metadata": {
        "id": "WkhJQG--nfZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bring in deps\n",
        "import os\n",
        "import streamlit as st\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, SequentialChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.utilities import WikipediaAPIWrapper"
      ],
      "metadata": {
        "id": "9SF0YeSynpq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Key Setup\n",
        "\n",
        "Here, we declare an API key variable and make it available as an environment variable, so that other packages, mainly the OpenAI package, have access to it.\n",
        "\n",
        "<br>\n",
        "We do so by using...\n",
        "\n",
        "```python\n",
        "os.environ['OPENAI_API_KEY'] = apikey\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1H84qADZoMFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apikey = \"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\"\n",
        "os.environ['OPENAI_API_KEY'] = apikey"
      ],
      "metadata": {
        "id": "2qeM4QlToyDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple UI Setup for Our App\n",
        "\n",
        "Here, we're just writing a few lines to set a title and placeholder text for our application. Streamlit takes care of the rest for us, as this is an \"AutoGPT\", meaning a lot of the heavy lifting is done under the hood."
      ],
      "metadata": {
        "id": "EgOEWExW99ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# App Framework\n",
        "st.title(\"üêç üîó Python Assistant\")\n",
        "prompt = st.text_input(\"Enter a Python topic here...\")"
      ],
      "metadata": {
        "id": "jBFZrI93-Wm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Configuration\n",
        "\n",
        "In this section, we're configuring the prompts that our LLM (large language model) will receive. Think of the prompt as something you'd manually type into ChatGPT yourself.\n",
        "\n",
        "<br>\n",
        "\n",
        "In the case of our Python Assistant app, our prompt will be something like...\n",
        "\n",
        "```python\n",
        "\"Explain {topic} in Python in simple terms\"\n",
        "```\n",
        "\n",
        "Where {topic} is what a user provides as input"
      ],
      "metadata": {
        "id": "jmSwe8v8-gfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt templates\n",
        "explanation_template = PromptTemplate(\n",
        "    input_variables = [\"topic\"],\n",
        "    template = \"Explain {topic} in Python in simple terms\"\n",
        ")"
      ],
      "metadata": {
        "id": "whHSXH0l_Ovu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory Configuration\n",
        "\n",
        "Here, we're giving our application the feature of memory. In other words, we're making it possible for our prompts and output to be stored for future usage. This is very useful for creating Chat based applications, like say an AI assistant or something like ChatGPT.\n",
        "\n",
        "<br>\n",
        "\n",
        "For instance, memory can be used to access a previous response which can then be used as part of the input for an upcoming prompt. In other words, you can use the result of 1 prompt as the input for another.\n",
        "\n",
        "<br>\n",
        "\n",
        "In the code snippet below, ```input_key``` is what's being stored and kept track of. In this case, ```input_key``` is the topic being explained.\n",
        "\n",
        "```memory_key``` is a title to a dictionary entry where responses are stored. In other words, somewhere we have a dictionary with a key by the name of ```explanation_history``` where a prompt response can be found. This is useful later on, for when we want to make use of Sequential Chains, or rather, sequential prompts."
      ],
      "metadata": {
        "id": "fKLdhW1KANZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory Setup\n",
        "explanation_memory = ConversationBufferMemory(input_key=\"topic\", memory_key=\"explanation_history\")"
      ],
      "metadata": {
        "id": "Ue74PKl5BVjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Setup with Chains\n",
        "\n",
        "Here, we're configuring a chain for running our prompt. Think of a chain as the actual request being made to the LLM. We provide a chain the LLM it should use, the prompt, a place to store responses (aka a memory) and other options. So in other words, it's more like a configuration for an LLM request.\n",
        "\n",
        "<br>\n",
        "\n",
        "A single chain on it's own is really just a call or request to an LLM. Where chains shine is when they're linked together and run sequentially. This, in conjunction with memory, makes the concept of continuity possible.\n",
        "\n",
        "<br>\n",
        "\n",
        "For example, let's say you gave your application a snippet of code and told your app to explain it. Your application explains the code, as intended, but now you also want it to simplify this code.\n",
        "\n",
        "Instead of having to reprompt your application and repaste your code, it'd be easier to just tell your application, \"okay, now simplify the code from before\" or something along those lines.\n",
        "\n",
        "This is done through the usage of memory and sequential chains. A sequential chain, as the name suggests, runs many other chains in a given order. Think of a Sequential Chain as a list of other chains.\n",
        "\n",
        "<br>\n",
        "\n",
        "The code snippet below sets up our app to use an OpenAI LLM under the hood, using the OpenAI package we installed before. We then set up a chain using the ```llm``` variable we setup before, along with the ```explanation_template``` & other variables from before"
      ],
      "metadata": {
        "id": "C58rEZRlDRjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This line configures our LLM using the OpenAI package we installed\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "# Here, we set up our chain, or request, using the many variables we created before\n",
        "explanation_chain = LLMChain(llm=llm, prompt=explanation_template, verbose=True, output_key=\"topic\", memory=explanation_memory)"
      ],
      "metadata": {
        "id": "OtJYV7KjFKD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying prompt output within our app\n",
        "\n",
        "In this section, we write some code that runs our prompt through an LLM and displays the output to our application.\n",
        "\n",
        "If there's valid user input within our text input box, it is used as part of the prompt we created. We then run this prompt, write the output, and keep track of the prompt as a whole in memory. This memory is then displayed in what looks like an accordion dropdown."
      ],
      "metadata": {
        "id": "VxXroW_PG2N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show stuff to the screen if there's a prompt\n",
        "if prompt:\n",
        "    explanation = explanation_chain.run(prompt)\n",
        "\n",
        "    st.write(explanation)\n",
        "\n",
        "    with st.expander('Explanation History'):\n",
        "        st.info(explanation_memory.buffer)"
      ],
      "metadata": {
        "id": "uNjKSx6LHH5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting The Puzzle Together\n",
        "\n",
        "The cell below combines every snippet of code from above into one large cell, and writes it out to a file to be run by ```streamlit``` & ```localtunnel``` via the terminal. Feel free to just hit run so that you can run all cells & see the complete application come alive!"
      ],
      "metadata": {
        "id": "lcI3mXWAHl5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary Python packages using pip\n",
        "!pip install -q langchain streamlit openai wikipedia chromadb tiktoken\n",
        "\n",
        "# Install localtunnel to serve Streamlit App, using NPM\n",
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "PvONemc0In7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# Bring in deps\n",
        "import os\n",
        "\n",
        "import streamlit as st\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, SequentialChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "apikey = \"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\"\n",
        "os.environ['OPENAI_API_KEY'] = apikey\n",
        "\n",
        "# App framework\n",
        "st.title(\"üêç üîó Python Assistant\")\n",
        "prompt = st.text_input(\"Enter a Python topic here...\")\n",
        "\n",
        "# Prompt templates\n",
        "explanation_template = PromptTemplate(\n",
        "    input_variables = [\"topic\"],\n",
        "    template = \"Explain {topic} in Python in simple terms\"\n",
        ")\n",
        "\n",
        "# Memory\n",
        "explanation_memory = ConversationBufferMemory(input_key=\"topic\", memory_key=\"explanation_history\")\n",
        "\n",
        "# Llms\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "# Chains\n",
        "explanation_chain = LLMChain(llm=llm, prompt=explanation_template, verbose=True, output_key=\"topic\", memory=explanation_memory)\n",
        "\n",
        "# Show stuff to the screen if there's a prompt\n",
        "if prompt:\n",
        "    explanation = explanation_chain.run(prompt)\n",
        "\n",
        "    st.write(explanation)\n",
        "\n",
        "\n",
        "    with st.expander('Explanation History'):\n",
        "        st.info(explanation_memory.buffer)"
      ],
      "metadata": {
        "id": "vtAqKowXIG17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "b0KAKUC-Iy4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "xlhpbmfbIz9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try It Yourself\n",
        "\n",
        "As a challenge, we'd love for you to repurpose this application to your liking. Maybe you'd like to make a more comprehensive Python assistant that also provides code examples. You can do so by building on top of this app!\n",
        "\n",
        "<br>\n",
        "\n",
        "Or maybe, you'd like to create an email writer that writes a sample email for you, along with a subject line and other details. You can do this with multiple prompts.\n",
        "\n",
        "<br>\n",
        "\n",
        "Better yet, you could even try using multiple chains in a sequential chain, to make use of previous prompt responses! The possibilities are endless here!"
      ],
      "metadata": {
        "id": "pR_lTomeOFHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary Python packages using pip\n",
        "!pip install -q langchain streamlit openai wikipedia chromadb tiktoken\n",
        "\n",
        "# Install localtunnel to serve Streamlit App, using NPM\n",
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "ku6WofmYPcEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile app.py\n",
        "# Before test running your app, make sure to uncomment the line above\n",
        "\n",
        "# Write all your application code here..."
      ],
      "metadata": {
        "id": "RhjVfewYPJuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "xJQS86EUPh7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "6GvbunItPkmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "- [Streamlit Documentation](https://docs.streamlit.io/)\n",
        "- [LangChain Documentation](https://python.langchain.com/en/latest/index.html)\n",
        "- [OpenAI Documentation](https://platform.openai.com/docs/api-reference)\n",
        "- [Python Documentation](https://docs.python.org/3/)\n",
        "\n",
        "- Video One: https://youtu.be/lnA9DMvHtfI\n",
        "\n",
        "- Video Two: https://www.youtube.com/watch?v=YDiSFS-yHwk"
      ],
      "metadata": {
        "id": "uxn0IoY0PqsF"
      }
    }
  ]
}